{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Analysis Guide for WiDS Workshop\n",
    "\n",
    "Hello and welcome to the **WiDS Causal Analysis Workshop**! Before we begin, make sure you have set up your environment as per the instructions in our `README.md` file on GitHub.\n",
    "\n",
    "This Jupyter Notebook is your one-stop guide to Causal Analysis, starting from **Causal Discovery** to **Causal Inference**. We’ll walk you through the entire process, which includes loading your dataset, training the causal discovery model (using [DECI](https://github.com/microsoft/causica/tree/main)/[DirectLiNGAM](https://lingam.readthedocs.io/en/latest/tutorial/lingam.html)), generating edge confidence for the causal graph, estimating errors for the fitted model, and finally, estimating Average Treatment Effect (ATE), Conditional Average Treatment Effect (CATE) and Individual Treatment Effect (ITE) using [DECI](https://github.com/microsoft/causica/tree/main)/[DML](https://econml.azurewebsites.net/spec/estimation/dml.html).\n",
    "\n",
    "Note: This notebook is adapted from [causica example notebook](https://github.com/microsoft/causica/blob/main/examples/multi_investment_sales_attribution.ipynb).\n",
    "\n",
    "By the end of this guide, you’ll have a solid understanding of causal analysis and be equipped to perform causal discovery and inference analysis on your own datasets. So, let’s get started!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "If you have not already set up your environment, please follow the instructions in `README.md` before proceeding. \n",
    "\n",
    "You can also download and install Graphviz on your system from https://graphviz.org/ and `pip install graphviz`  for improved visualisations of the causal graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "import fsspec\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "from tensordict import TensorDict\n",
    "\n",
    "from causica.datasets.causica_dataset_format import CAUSICA_DATASETS_PATH, Variable\n",
    "from causica.distributions import ContinuousNoiseDist\n",
    "from causica.lightning.data_modules.basic_data_module import BasicDECIDataModule\n",
    "from causica.lightning.modules.deci_module import DECIModule\n",
    "from causica.sem.sem_distribution import SEMDistributionModule\n",
    "from causica.sem.structural_equation_model import ite\n",
    "from causica.training.auglag import AugLagLRConfig\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "test_run = bool(os.environ.get(\"TEST_RUN\", False))  # used by testing to run the notebook as a script\n",
    "DATADOWNLOAD = True\n",
    "DATA_PATH = './data/multi_attribution_data_20220819_data.csv' # from https://azuastoragepublic.z6.web.core.windows.net/causal_ai_suite/multi_attribution_data_20220819_data.csv\n",
    "VARIABLES_PATH = './data/multi_attribution_data_20220819_data_types.json' # from https://azuastoragepublic.z6.web.core.windows.net/causal_ai_suite/multi_attribution_data_20220819_data_types.json\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Example Dataset Overview: Multi-investment Attribution: Distinguish the Effects of Multiple Outreach Efforts\n",
    "\n",
    "<img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/05/Attribution.png\" width=\"250\">\n",
    "\n",
    "A software company would like to know whether its multiple outreach efforts to their business customers are successful in boosting sales. They would also like to learn how to better target different incentives to different customers. In other words, they would like to learn the **treatment effect** of each investment on customers' total expenditure on the company’s products: particularly the **heterogeneous treatment effect**. \n",
    "\n",
    "In an ideal world, the company would run experiments where each customer would receive a random assortment of investments. However, this approach can be logistically prohibitive or strategically unsound: the company might not have the resources to design such experiments or they might not want to risk major revenue losses by randomizing their incentives to top customers.\n",
    "\n",
    "In this notebook, we show how tools from the [Causica](https://github.com/microsoft/causica), library can use historical investment data to learn the effects of multiple investments.\n",
    "\n",
    "For this exercise, we create simulated data that recreates some key characteristics of real data from a software company facing this type of decision. Simulating data protects the company’s privacy. Because we create the data, we are also in the unusual position of knowing the true causal graph and true effects of each investments, so we can compare the results of our estimation to this ground truth.\n",
    "\n",
    "In the next section, we introduce this simulated data. We then discover the causal graph, the relationship between each variable in the simulated data. We use this generated graph, to estimate the treatment effects. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The simulated dataset contains 10,000 customers. \n",
    "\n",
    "We create one outcome of interest:\n",
    "\n",
    "Feature Name | Type | Details \n",
    ":--- |:--- |:--- \n",
    "**Revenue** | continuous | \\$ Annual revenue from customer given by the amount of software purchased\n",
    "\n",
    "We consider three possible treatments, the interventions whose impact we wish to measure:\n",
    "\n",
    "Feature Name | Type | Details \n",
    ":--- |:--- |:--- \n",
    "**Tech Support** | binary | whether the customer received tech support during the year\n",
    "**Discount** | binary | whether the customer was given a discount during the year\n",
    "**New Strategy** | binary | whether the customer was targeted for a new engagement strategy with different outreach behaviors\n",
    "\n",
    "Finally, we consider a variety of additional customer characteristics that may affect revenue. Including these types of features is crucial for causal analysis in order to map the full causal graph and separate the true effects of treatments on outcomes from other correlation generated by other influences. \n",
    "\n",
    "Feature Name | Type | Details \n",
    ":--- |:--- |:--- \n",
    "**Global Flag** | binary | whether the customer has global offices\n",
    "**Major Flag** | binary | whether the customer is a large consumer in their industry (as opposed to SMC - Small Medium Corporation - or SMB - Small Medium Business)\n",
    "**SMC Flag** | binary | whether the customer is a Small Medium Corporation (SMC, as opposed to major and SMB)\n",
    "**Commercial Flag** | binary | whether the customer's business is commercial (as opposed to public sector)\n",
    "**Planning Summit** | binary | whether a sales team member held an outreach event with the customer during the year\n",
    "**New Product Adoption** | binary | whether the customer signed a contract for any new products during the year\n",
    "**IT Spend** | continuous | \\$ spent on IT-related purchases \n",
    "**Employee Count** | continuous | number of employees\n",
    "**PC Count** | continuous | number of PCs used by the customer\n",
    "**Size** | continuous | customer's total revenue in the previous calendar year\n",
    "\n",
    "In simulating the data, we maintain some key characteristics of the data from the real company example, including some correlation patterns between features and some potentially difficult data characteristics, such as large outliers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "with fsspec.open(VARIABLES_PATH, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    variables_spec = json.load(f)[\"variables\"]\n",
    "\n",
    "data_module = BasicDECIDataModule(\n",
    "    df.loc[:, \"Global Flag\":\"Revenue\"], # remove ground truth from dataframe for simulated data\n",
    "    variables=[Variable.from_dict(d) for d in variables_spec],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    normalize=True,\n",
    ")    \n",
    "num_nodes = len(data_module.dataset_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data sample\n",
    "print(\"Data Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let’s construct the Causal Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have a sheet of paper in front of you, with all the variables from the dataset represented as nodes. Currently, this graph has no edges.\n",
    "\n",
    "<img src=\"./img/DrawCausalGraph.png\" width=\"800\">\n",
    "\n",
    " Your task is to draw edges between the nodes (variables) that you believe have a causal relationship. Remember, the direction of the edge is important, so make sure to indicate it. Please don't draw a fully connected graph :)\n",
    "\n",
    " **Please ensure your graph does not have cycles**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover the Causal Graph using DECI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Adding prior domain knowledge\n",
    "#### Note : Provided constraints are for Simulated data. You will need to create appropriate constraints as required for the real data\n",
    "To improve the quality of the learned graph, it is possible to place constraints on the graph that DECI learns. The constraints come in two flavours:\n",
    " - *negative constraints* mean a certain edge cannot exist in the graph,\n",
    " - *positive constraints* mean a certain edge must exist in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The constraint matrix has the same shape as the adjacency matrix\n",
    "# A `nan` value means no constraint\n",
    "# A 0 value means a negative constraint\n",
    "# A 1 value means a positive constraint\n",
    "node_name_to_idx = {key: i for i, key in enumerate(data_module.dataset_train.keys())}\n",
    "constraint_matrix = np.full((num_nodes, num_nodes), np.nan, dtype=np.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we introduce the constraint that Revenue cannot be the cause of any other node, except possibly Planning Summit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_idx = node_name_to_idx[\"Revenue\"]\n",
    "planning_summit_idx = node_name_to_idx[\"Planning Summit\"]\n",
    "constraint_matrix[revenue_idx, :] = 0.0\n",
    "constraint_matrix[revenue_idx, planning_summit_idx] = np.nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we say that certain basic attributes of companies cannot be changed by other variables (at least on the time scale we are considering). The attributes we constraint to have no parents are: Commerical Flag, Major Flag, SMC Flag, PC Count, Employee Count, Global Flag, Size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_child_nodes = [\n",
    "    \"Commercial Flag\",\n",
    "    \"Major Flag\",\n",
    "    \"SMC Flag\",\n",
    "    \"PC Count\",\n",
    "    \"Employee Count\",\n",
    "    \"Global Flag\",\n",
    "    \"Size\",\n",
    "]\n",
    "non_child_idxs = itemgetter(*non_child_nodes)(node_name_to_idx)\n",
    "constraint_matrix[:, non_child_idxs] = 0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we make a constraint that says that different engagements do not directly cause one another. For example, giving Tech Support to a company is not a valid reason to give / not give them a Discount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engagement_nodes = [\"Tech Support\", \"Discount\", \"New Engagement Strategy\"]\n",
    "engagement_idxs = itemgetter(*engagement_nodes)(node_name_to_idx)\n",
    "for i in engagement_idxs:\n",
    "    constraint_matrix[engagement_idxs, i] = 0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and training DECI model\n",
    "\n",
    "The following snippets step through the process of:\n",
    " - configuring and creating a DECI model\n",
    " - training a model with graph constraints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECI configuration\n",
    "\n",
    "The DECI model has a number of hyperparameters, but attention need not be paid to all of them. Here we highlight key hyperparameters that might be changed to improve performance:\n",
    " - `noise_dist` is the type of DECI model that is trained with. It should be either Gaussian or Spline. Use a Spline for highly non-Gaussian data, or to fit a better density model of the observational data.\n",
    " \n",
    "Other hyperparameters are less frequently changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(seed=4)  # set the random seed\n",
    "\n",
    "lightning_module = DECIModule(\n",
    "    noise_dist=ContinuousNoiseDist.GAUSSIAN,\n",
    "    prior_sparsity_lambda=43.0,\n",
    "    init_rho=30.0,\n",
    "    init_alpha=0.20,\n",
    "    auglag_config=AugLagLRConfig(\n",
    "        max_inner_steps=3400,\n",
    "        max_outer_steps=8,\n",
    "        lr_init_dict={\n",
    "            \"icgnn\": 0.00076,\n",
    "            \"vardist\": 0.0098,\n",
    "            \"functional_relationships\": 3e-4,\n",
    "            \"noise_dist\": 0.0070,\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "\n",
    "lightning_module.constraint_matrix = torch.tensor(constraint_matrix)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs=2000,\n",
    "    fast_dev_run=test_run,\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=19)],\n",
    "    enable_checkpointing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(lightning_module, datamodule=data_module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading a DECI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lightning_module.sem_module, \"deci.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_module: SEMDistributionModule = torch.load(\"deci.pt\")\n",
    "\n",
    "# create a structural equation model using the most likely graph\n",
    "sem = sem_module().mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Causal Graph using \"Mode\" of the posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.from_numpy_array(sem.graph.cpu().numpy(), create_using=nx.DiGraph)\n",
    "graph = nx.relabel_nodes(graph, dict(enumerate(data_module.dataset_train.keys())))\n",
    "\n",
    "fig, axis = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "labels = {node: i for i, node in enumerate(graph.nodes)}\n",
    "\n",
    "# layout = nx.nx_agraph.graphviz_layout(graph, prog=\"dot\")\n",
    "layout = nx.circular_layout(graph)\n",
    "\n",
    "for node, i in labels.items():\n",
    "    axis.scatter(layout[node][0], layout[node][1], label=f\"{i}: {node}\")\n",
    "axis.legend(loc=\"center left\", bbox_to_anchor=(1.05, 0.5))\n",
    "\n",
    "nx.draw_networkx(graph, pos=layout, with_labels=True, arrows=True, labels=labels, ax=axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to Ground Truth Causal Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adjacency_path = \"./data/true_graph_gml_string.txt\"\n",
    "with fsspec.open(adjacency_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    true_adj = nx.parse_gml(f.read())\n",
    "\n",
    "fig, axis = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "# try:\n",
    "#     layout = nx.nx_agraph.graphviz_layout(true_adj, prog=\"dot\")\n",
    "# except (ModuleNotFoundError, ImportError):\n",
    "#     layout = nx.circular_layout(graph)\n",
    "\n",
    "for node, i in labels.items():\n",
    "    axis.scatter(layout[node][0], layout[node][1], label=f\"{i}: {node}\")\n",
    "axis.legend(loc=\"center left\", bbox_to_anchor=(1.05, 0.5))\n",
    "\n",
    "nx.draw_networkx(true_adj, pos=layout, with_labels=True, arrows=True, labels=labels, ax=axis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluding graph discovery\n",
    "The DECI model from Causica offers us a way to discover a causal graph from observational data. The learned graph can be iteratively refined by adding new graph constraints and retraining DECI to obtain a more realistic graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# (Alternative) - Discover the Causal Graph using DirectLiNGAM \n",
    "In case you have **trouble installing DECI**, we can use DirectLiNGAM method which also accepts prior domain constraints.\n",
    "\n",
    "Note: If you **ran DECI successfully**, then please **skip this session** and go ahead to the Treatment Effect Estimation section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding prior domain knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import lingam\n",
    "from lingam.utils import make_dot\n",
    "\n",
    "print([np.__version__, pd.__version__, graphviz.__version__, lingam.__version__])\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "0: No direct path from variable i to variable j\n",
    "\n",
    "1: Directed path exists from variable i and variable j\n",
    "\n",
    "-1: No prior knowledge is available to know if either of the two cases above (0 or 1) is true.\n",
    "\n",
    "'''\n",
    "node_name_to_idx = {key: i for i, key in enumerate(df.columns)}\n",
    "prior_knowledge = -1 * np.ones((len(df.loc[:, \"Global Flag\":\"Revenue\"].columns), len(df.loc[:, \"Global Flag\":\"Revenue\"].columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we introduce the constraint that Revenue cannot be the cause of any other node, except possibly Planning Summit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_idx = node_name_to_idx[\"Revenue\"]\n",
    "planning_summit_idx = node_name_to_idx[\"Planning Summit\"]\n",
    "prior_knowledge[:, revenue_idx] = 0.0\n",
    "prior_knowledge[planning_summit_idx, revenue_idx] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we say that certain basic attributes of companies cannot be changed by other variables (at least on the time scale we are considering). The attributes we constraint to have no parents are: Commerical Flag, Major Flag, SMC Flag, PC Count, Employee Count, Global Flag, Size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_child_nodes = [\n",
    "    \"Commercial Flag\",\n",
    "    \"Major Flag\",\n",
    "    \"SMC Flag\",\n",
    "    \"PC Count\",\n",
    "    \"Employee Count\",\n",
    "    \"Global Flag\",\n",
    "    \"Size\",\n",
    "]\n",
    "non_child_idxs = itemgetter(*non_child_nodes)(node_name_to_idx)\n",
    "prior_knowledge[non_child_idxs, :] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we make a constraint that says that different engagements do not directly cause one another. For example, giving Tech Support to a company is not a valid reason to give / not give them a Discount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engagement_nodes = [\"Tech Support\", \"Discount\", \"New Engagement Strategy\"]\n",
    "engagement_idxs = itemgetter(*engagement_nodes)(node_name_to_idx)\n",
    "for i in engagement_idxs:\n",
    "    prior_knowledge[i, engagement_idxs] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the Causal Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lingam.DirectLiNGAM(prior_knowledge=prior_knowledge)\n",
    "model.fit(df.loc[:, \"Global Flag\":\"Revenue\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Causal Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = model.adjacency_matrix_.copy().T\n",
    "graph = nx.from_numpy_array(adj_matrix, create_using=nx.DiGraph)\n",
    "graph = nx.relabel_nodes(graph, dict(enumerate(df.columns)))\n",
    "\n",
    "fig, axis = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "labels = {node: i for i, node in enumerate(graph.nodes)}\n",
    "layout = nx.circular_layout(graph)\n",
    "\n",
    "for node, i in labels.items():\n",
    "    axis.scatter(layout[node][0], layout[node][1], label=f\"{i}: {node}\")\n",
    "axis.legend(loc=\"center left\", bbox_to_anchor=(1.05, 0.5))\n",
    "\n",
    "nx.draw_networkx(graph, pos=layout, with_labels=True, arrows=True, labels=labels, ax=axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to Ground Truth Causal Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_path = \"./data/true_graph_gml_string.txt\"\n",
    "with fsspec.open(adjacency_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    true_adj = nx.parse_gml(f.read())\n",
    "\n",
    "fig, axis = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "# try:\n",
    "#     layout = nx.nx_agraph.graphviz_layout(true_adj, prog=\"dot\")\n",
    "# except (ModuleNotFoundError, ImportError):\n",
    "#     layout = nx.circular_layout(graph)\n",
    "\n",
    "for node, i in labels.items():\n",
    "    axis.scatter(layout[node][0], layout[node][1], label=f\"{i}: {node}\")\n",
    "axis.legend(loc=\"center left\", bbox_to_anchor=(1.05, 0.5))\n",
    "\n",
    "nx.draw_networkx(true_adj, pos=layout, with_labels=True, arrows=True, labels=labels, ax=axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Did you get the edges right?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treatment effect estimation\n",
    "\n",
    "The causal graph identifies the likely paths of connection between features. This next step will quantify the strength of those relationships between treatment and outcomes. Our tools estimate both the average treatment effect across all customers (ATE) and how these treatment effects vary across customer features (heterogeneous treatment effect, HTE). If we are confident that we have included every feature that affects treatment intensity, we can also interpret these estimates as individual treatment effects (ITE), or the effect of treating a specific customer.\n",
    "\n",
    "We will use two different methods for estimation (DECI and Double ML) and compare how well they do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the Ground Truth Causal Effects\n",
    "\n",
    "Unlike most real-world causal use cases, in this case we know the true causal relationships between treatments and outcome because we have simulated the data.\n",
    "\n",
    "We also extract those effects, created during data generation, to check our later causal effect estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = \"Revenue\"\n",
    "treatment_columns = [\"Tech Support\", \"Discount\", \"New Engagement Strategy\"]\n",
    "\n",
    "# extract ground truth effects\n",
    "ground_truth_effects = df.loc[:, \"Direct Treatment Effect: Tech Support\":]\n",
    "\n",
    "ground_truth_ites = {\n",
    "    treatment: ground_truth_effects[f\"Total Treatment Effect: {treatment}\"] for treatment in treatment_columns\n",
    "}\n",
    "\n",
    "ground_truth_ates = {key: val.mean(axis=0) for key, val in ground_truth_ites.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Of the three treatments, which do you think is most effective?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Treatment Effect using DECI (ATE)\n",
    "We can estimate the average treatment and estimate the error using samples from the intervened SEM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = \"Revenue\"\n",
    "treatment_columns = [\"Tech Support\", \"Discount\", \"New Engagement Strategy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_estimated_ate = {}\n",
    "num_samples = 20000\n",
    "sample_shape = torch.Size([num_samples])\n",
    "normalizer = data_module.normalizer\n",
    "\n",
    "# extract estimated ATEs from DECI\n",
    "for treatment in treatment_columns:\n",
    "    intervention_a = TensorDict({treatment: torch.tensor([1.0])}, batch_size=tuple())\n",
    "    intervention_b = TensorDict({treatment: torch.tensor([0.0])}, batch_size=tuple())\n",
    "\n",
    "    rev_a_samples = normalizer.inv(sem.do(interventions=intervention_a).sample(sample_shape))[outcome]\n",
    "    rev_b_samples = normalizer.inv(sem.do(interventions=intervention_b).sample(sample_shape))[outcome]\n",
    "\n",
    "    ate_mean = rev_a_samples.mean(0) - rev_b_samples.mean(0)\n",
    "    ate_std = np.sqrt((rev_a_samples.var(0) + rev_b_samples.var(0)) / num_samples)\n",
    "\n",
    "    revenue_estimated_ate[treatment] = (\n",
    "        ate_mean.cpu().numpy()[0],\n",
    "        ate_std.cpu().numpy()[0],\n",
    "    )\n",
    "revenue_estimated_ate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the estimated average treatment effects of DECI with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(ground_truth_ates), figsize=(16, 4))\n",
    "fig.suptitle(\"Comparison of Ground Truth ATEs with DECI estimates\")\n",
    "for ax, treatment in zip(axes, revenue_estimated_ate.keys()):\n",
    "    ax.errorbar([0], [revenue_estimated_ate[treatment][0]], [2 * revenue_estimated_ate[treatment][1]], fmt=\"o\")\n",
    "    ax.scatter([0], [ground_truth_ates[treatment]], color=\"r\", label=\"Ground Truth\")\n",
    "    ax.legend()\n",
    "    ax.set_title(treatment)\n",
    "    ax.set_ylabel(\"Estimated ATE on Revenue ($)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: How well did DECI estimate the ATEs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Treatment Effect using DML (ATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from econml.dml import LinearDML\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Double ML, let's estimate the ATE for each intervention and compare to the ground truth and DECI estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_estimated_ate_econml = {}\n",
    "\n",
    "# prepare treatment, outcome, and confounder data\n",
    "Y = df[\"Revenue\"]  # outcome\n",
    "W = df[[\"SMC Flag\", \"Major Flag\", \"Commercial Flag\", \"Employee Count\", \"PC Count\", \"IT Spend\"]] # confounders\n",
    "\n",
    "# set first-stage models\n",
    "outcome_model = RandomForestRegressor(random_state=100, max_depth=5) # predict outcome from confounders\n",
    "treatment_model = RandomForestClassifier(random_state=100, max_depth=5) # predict treatment from confounders\n",
    "\n",
    "# run DML for each treatment\n",
    "for treatment in treatment_columns:\n",
    "    T = df[treatment]  # intervention\n",
    "\n",
    "    est = LinearDML(model_y=outcome_model,\n",
    "                model_t=treatment_model,\n",
    "                discrete_treatment=True, # specify that treatment should be treated as categorical\n",
    "                categories=[0,1], # specify the category labels of the treatment\n",
    "                random_state=123) # set seed for reproducibility\n",
    "    est.fit(Y=Y, T=T, W=W, X=None)\n",
    "    ate = est.effect_inference().point_estimate\n",
    "    ate_se = est.effect_inference().stderr\n",
    "\n",
    "    # save output\n",
    "    revenue_estimated_ate_econml[treatment] = (\n",
    "    ate,\n",
    "    ate_se\n",
    "    )\n",
    "\n",
    "revenue_estimated_ate_econml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look at the output for New Engagement Strategy and interpret the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.summary(alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: The point estimate is the estimated ATE of New Engagement Strategy on Revenue and the summary table includes inference results.**\n",
    "\n",
    "**Do we have evidence that New Engagement Strategy is effective in moving Revenue?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's compare the estimated ATE of each treatment to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(ground_truth_ates), figsize=(16, 4))\n",
    "fig.suptitle(\"Comparison of Ground Truth ATEs with DECI & DML estimates\")\n",
    "for ax, treatment in zip(axes, revenue_estimated_ate_econml.keys()):\n",
    "    ax.errorbar([0], [revenue_estimated_ate[treatment][0]], [2 * revenue_estimated_ate[treatment][1]], fmt=\"o\")\n",
    "    ax.scatter([1], [ground_truth_ates[treatment]], color=\"r\", label=\"Ground Truth\")\n",
    "    ax.errorbar([2], [revenue_estimated_ate_econml[treatment][0]], [2 * revenue_estimated_ate_econml[treatment][1]], fmt=\"o\")\n",
    "    ax.set_xticks(np.arange(3))\n",
    "    ax.set_xticklabels(['DECI'] + ['GroundTruth'] + ['DML'])\n",
    "    ax.legend()\n",
    "    ax.set_title(treatment)\n",
    "    ax.set_ylabel(\"Estimated ATE on Revenue ($)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: How well did Double ML estimate the ATEs compared to DECI?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Try adding/removing confounders. How does the estimated ATE change?\n",
    "You can try with the confounders from your DECI/DirectLiNGAM output graph and,\n",
    "\n",
    "you can also try with your own set of confounders from your drawing sheet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_estimated_ate_confounderchanged = {}\n",
    "\n",
    "# prepare treatment, outcome, and confounder data\n",
    "Y = df[\"Revenue\"]  # outcome\n",
    "# W = df[[]] #  -----------------------------------------------> Add your confounders in the list e.g df[[\"SMC Flag\", \"Major Flag\"]]\n",
    "\n",
    "# set first-stage models\n",
    "outcome_model = RandomForestRegressor(random_state=100, max_depth=5) # predict outcome from confounders\n",
    "treatment_model = RandomForestClassifier(random_state=100, max_depth=5) # predict treatment from confounders\n",
    "\n",
    "# run DML for each treatment\n",
    "for treatment in treatment_columns:\n",
    "    T = df[treatment]  # intervention\n",
    "\n",
    "    est = LinearDML(model_y=outcome_model,\n",
    "                model_t=treatment_model,\n",
    "                discrete_treatment=True, # specify that treatment should be treated as categorical\n",
    "                categories=[0,1], # specify the category labels of the treatment\n",
    "                random_state=123) # set seed for reproducibility\n",
    "    est.fit(Y=Y, T=T, W=W, X=None)\n",
    "    ate = est.effect_inference().point_estimate\n",
    "    ate_se = est.effect_inference().stderr\n",
    "\n",
    "    # save output\n",
    "    revenue_estimated_ate_confounderchanged[treatment] = (\n",
    "    ate,\n",
    "    ate_se\n",
    "    )\n",
    "\n",
    "revenue_estimated_ate_confounderchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(ground_truth_ates), figsize=(18, 7))\n",
    "fig.suptitle(\"Comparison of Ground Truth ATEs with DECI & DML estimates\")\n",
    "for ax, treatment in zip(axes, revenue_estimated_ate_econml.keys()):\n",
    "    ax.scatter([0], [ground_truth_ates[treatment]], color=\"r\", label=\"Ground Truth\")\n",
    "    ax.errorbar([1], [revenue_estimated_ate[treatment][0]], [2 * revenue_estimated_ate[treatment][1]], fmt=\"o\")\n",
    "    ax.errorbar([2], [revenue_estimated_ate_econml[treatment][0]], [2 * revenue_estimated_ate_econml[treatment][1]], fmt=\"o\")\n",
    "    # ax.errorbar([3], [revenue_estimated_ate_confounderchanged[treatment][0]], [2 * revenue_estimated_ate_confounderchanged[treatment][1]], fmt=\"o\") #-----------------------> uncomment\n",
    "    # ax.set_xticks(np.arange(4)) #-----------------------> uncomment\n",
    "    # ax.set_xticklabels(['GroundTruth'] + ['DECI'] + ['DML RF True Confounder'] + ['DML RF Modified Confounder'], rotation=90) #-----------------------> uncomment\n",
    "    ax.set_title(treatment)\n",
    "    ax.set_ylabel(\"Estimated ATE on Revenue ($)\")\n",
    "    ax.grid()\n",
    "    \n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Try using different first-stage ML models\n",
    "Note : You can use any sklearn model.\n",
    "You can also use LightGBM model and import it as `from lightgbm import LGBMClassifier, LGBMRegressor`\n",
    "\n",
    "**How does the estimated ATE change?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression, LinearRegression # -----------------------> uncomment\n",
    "\n",
    "revenue_estimated_ate_lr = {}\n",
    "\n",
    "# prepare treatment, outcome, and confounder data\n",
    "T = df['Discount']\n",
    "Y = df[\"Revenue\"]  # outcome\n",
    "W = df[[\"SMC Flag\", \"Major Flag\", \"Commercial Flag\", \"Employee Count\", \"PC Count\", \"IT Spend\"]] # confounders\n",
    "\n",
    "# set first-stage models\n",
    "# outcome_model = LinearRegression() # predict outcome from confounders -----------------------> uncomment\n",
    "# treatment_model = LogisticRegression() # predict treatment from confounders -----------------------> uncomment\n",
    "\n",
    "# run DML for each treatment\n",
    "for treatment in treatment_columns:\n",
    "    T = df[treatment]  # intervention\n",
    "\n",
    "    est = LinearDML(model_y=outcome_model,\n",
    "                model_t=treatment_model,\n",
    "                discrete_treatment=True, # specify that treatment should be treated as categorical\n",
    "                categories=[0,1], # specify the category labels of the treatment\n",
    "                random_state=123) # set seed for reproducibility\n",
    "    est.fit(Y=Y, T=T, W=W, X=None)\n",
    "    ate = est.effect_inference().point_estimate\n",
    "    ate_se = est.effect_inference().stderr\n",
    "\n",
    "    # save output\n",
    "    revenue_estimated_ate_lr[treatment] = (\n",
    "    ate,\n",
    "    ate_se\n",
    "    )\n",
    "\n",
    "revenue_estimated_ate_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(ground_truth_ates), figsize=(18, 7))\n",
    "fig.suptitle(\"Comparison of Ground Truth ATEs with DECI & DML estimates\")\n",
    "for ax, treatment in zip(axes, revenue_estimated_ate_econml.keys()):\n",
    "    ax.scatter([0], [ground_truth_ates[treatment]], color=\"r\", label=\"Ground Truth\")\n",
    "    ax.errorbar([1], [revenue_estimated_ate[treatment][0]], [2 * revenue_estimated_ate[treatment][1]], fmt=\"o\")\n",
    "    ax.errorbar([2], [revenue_estimated_ate_econml[treatment][0]], [2 * revenue_estimated_ate_econml[treatment][1]], fmt=\"o\")\n",
    "    ax.errorbar([3], [revenue_estimated_ate_confounderchanged[treatment][0]], [2 * revenue_estimated_ate_confounderchanged[treatment][1]], fmt=\"o\") \n",
    "    # ax.errorbar([4], [revenue_estimated_ate_lr[treatment][0]], [2 * revenue_estimated_ate_lr[treatment][1]], fmt=\"o\") # -----------------------> uncomment\n",
    "    # ax.set_xticks(np.arange(5)) # -----------------------> uncomment\n",
    "    # ax.set_xticklabels(['GroundTruth'] + ['DECI'] + ['DML RF True Confounder'] + ['DML RF Modified Confounder'] + ['DML LR True Confounder'], rotation=90) # -----------------------> uncomment\n",
    "    ax.set_title(treatment)\n",
    "    ax.set_ylabel(\"Estimated ATE on Revenue ($)\")\n",
    "    ax.grid()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating heterogeneous treatment effect (HTE) with DML\n",
    "\n",
    "Suppose the effect of the discount is more effective among certain types of businesses. Knowing this would allow us to optimize which customers should receive which treatments. Let's estimate the treatment effect of Discount for different customer sizes. Here, 'Size' is called an effect modifier, as it modifies the effectiveness of the treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = df[\"Tech Support\"]  # intervention, treatment\n",
    "Y = df[\"Revenue\"]  # amount of product purchased, outcome\n",
    "W = df[[\"SMC Flag\", \"Major Flag\", \"Commercial Flag\", \"Employee Count\", \"PC Count\", \"IT Spend\"]]\n",
    "X = df[[\"Size\"]] # effect modifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_model = RandomForestRegressor(random_state=100, max_depth=5) # predict outcome from confounders\n",
    "treatment_model = RandomForestClassifier(random_state=100, max_depth=5) # predict treatment from confounders\n",
    "\n",
    "est = LinearDML(model_y=outcome_model,\n",
    "                model_t=treatment_model,\n",
    "                discrete_treatment=True, # specify that treatment should be treated as categorical\n",
    "                categories=[0,1], # specify the categories of the treatment\n",
    "                random_state=123) # set seed for reproducibility\n",
    "est.fit(Y=Y, T=T, W=W, X=X)\n",
    "est.summary(alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The estimated effect of tech support (conditioned on company size) is:\", est.intercept_, \"+\", est.coef_[0], \"*size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "If a company has 0 previous year revenue (Size=0), then offering tech support to them will increase our revenue through software purchases by ~$6924 .\n",
    "\n",
    "For every `$1` increase in the company's previous year Revenue (Size), we expect that offering tech support will further increase their software purchases by ~$0.02.\n",
    "\n",
    "e.g. for a company of size 1 million, offering tech support will increase our revenue by ~$6924 + 0.02 * 1 million.\n",
    "\n",
    "=> Tech Support is more effective for companies with larger previous year revenue (indicaed by variable size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Estimating individual treatment effect (ITE) with DECI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_estimated_ite = {}\n",
    "\n",
    "base_noise = sem.sample_to_noise(data_module.dataset_train)\n",
    "\n",
    "for treatment in treatment_columns:\n",
    "    do_sem = sem.do(interventions=TensorDict({treatment: torch.tensor([1.0])}, batch_size=tuple()))\n",
    "    do_a_cfs = normalizer.inv(do_sem.noise_to_sample(base_noise))[outcome].cpu().detach().numpy()[:, 0]\n",
    "    do_sem = sem.do(interventions=TensorDict({treatment: torch.tensor([0.0])}, batch_size=tuple()))\n",
    "    do_b_cfs = normalizer.inv(do_sem.noise_to_sample(base_noise))[outcome].cpu().detach().numpy()[:, 0]\n",
    "    revenue_estimated_ite[treatment] = do_a_cfs - do_b_cfs\n",
    "\n",
    "revenue_estimated_ite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(ground_truth_ates), figsize=(16, 4))\n",
    "fig.suptitle(\"Comparison of Ground Truth ITEs with DECI estimates\")\n",
    "for ax, treatment in zip(axes, revenue_estimated_ite.keys()):\n",
    "    ax.scatter(revenue_estimated_ite[treatment], ground_truth_ites[treatment])\n",
    "    ax.set_title(treatment)\n",
    "    ax.set_xlabel(\"Estimated ITE ($)\")\n",
    "    ax.set_ylabel(\"Ground Truth ITE ($)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Advanced Evaluations for DECI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Edge Confidence estimate using samples from posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100\n",
    "adj_matrix_sum = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "for i in range(sample_size):\n",
    "\n",
    "    # Sample a graph from the posterior distribution\n",
    "    adj_matrix_sum += sem_module()._adjacency_dist.sample().numpy()\n",
    "\n",
    "# modify adj_matrix_sum such that it's a DAG. adj_matrix_sum should not have [x,y]>0 and [y,x]>0 at the same time for any x and y. If it does, keep the maximum value and set the other value to 0.\n",
    "for i in range(num_nodes):\n",
    "    for j in range(num_nodes):\n",
    "        if adj_matrix_sum[i][j] > adj_matrix_sum[j][i]:\n",
    "            adj_matrix_sum[j][i] = 0\n",
    "        else:\n",
    "            adj_matrix_sum[i][j] = 0\n",
    "\n",
    "adj_matrix_avg = adj_matrix_sum / sample_size\n",
    "\n",
    "print(adj_matrix_sum)\n",
    "print(adj_matrix_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the average adjacency matrix\n",
    "adj_matrix_avg[adj_matrix_avg < 0.5] = 0 # Only keep edges with weight >= 0.5\n",
    "\n",
    "graph = nx.from_numpy_array(adj_matrix_avg, create_using=nx.DiGraph)\n",
    "graph = nx.relabel_nodes(graph, dict(enumerate(data_module.dataset_train.keys())))\n",
    "\n",
    "labels = {node: i for i, node in enumerate(graph.nodes)}\n",
    "\n",
    "layout = nx.circular_layout(graph)\n",
    "# layout = nx.nx_agraph.graphviz_layout(graph, prog=\"dot\")\n",
    "\n",
    "fig, axis = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "for node, i in labels.items():\n",
    "    axis.scatter(layout[node][0], layout[node][1], label=f\"{i}: {node}\")\n",
    "\n",
    "# add legend to right side of the plot outside the plot area\n",
    "axis.legend(loc=\"center left\", bbox_to_anchor=(1.05, 0.5))\n",
    "\n",
    "nx.draw_networkx(graph, pos=layout, with_labels=True, arrows=True, labels=labels, ax=axis)  \n",
    "\n",
    "# Get weights of each edge and assign to labels\n",
    "labels = nx.get_edge_attributes(graph, \"weight\")\n",
    "\n",
    "# Draw edge labels using layout and list of labels\n",
    "nx.draw_networkx_edge_labels(graph, pos=layout, edge_labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate Edge Confidence from DECI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_lower_tri_elements_to_n(x: int) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the size of the matrix from the number of strictly lower triangular elements.\n",
    "\n",
    "    We have x = n(n - 1) / 2 for some n\n",
    "    n² - n - 2x = 0\n",
    "    so n = (1 + √(1 + 8x)) / 2\n",
    "    \"\"\"\n",
    "    val = int(np.sqrt(1 + 8 * x) + 1) // 2\n",
    "    if val * (val - 1) != 2 * x:\n",
    "        raise ValueError(\"Invalid number of lower triangular elements\")\n",
    "    return val\n",
    "\n",
    "\n",
    "def fill_triangular(vec: torch.Tensor, upper: bool = False) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vec: A tensor of shape (..., n(n-1)/2)\n",
    "        upper: whether to fill the upper or lower triangle\n",
    "    Returns:\n",
    "        An array of shape (..., n, n), where the strictly upper (lower) triangle is filled from vec\n",
    "        with zeros elsewhere\n",
    "    \"\"\"\n",
    "    num_nodes = num_lower_tri_elements_to_n(vec.shape[-1])\n",
    "    idxs = torch.triu_indices(num_nodes, num_nodes, offset=1, device=vec.device)\n",
    "    output = torch.zeros(vec.shape[:-1] + (num_nodes, num_nodes), device=vec.device)\n",
    "    output[..., idxs[0, :], idxs[1, :]] = vec\n",
    "    return output if upper else output.transpose(-1, -2)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Construct the matrix logit(pᵢⱼ).\n",
    "\n",
    "\n",
    "See the class docstring.\n",
    "We use the following derivation\n",
    "    logit(pᵢⱼ) = - log(pᵢⱼ⁻¹ - 1)\n",
    "                = - log((1 + exp(-γᵢⱼ))(1 + exp(-θᵢⱼ)) - 1)\n",
    "                = - log(exp(-γᵢⱼ) + exp(-θᵢⱼ) + exp(-θᵢⱼ - γᵢⱼ))\n",
    "                = - logsumexp([-γᵢⱼ, -θᵢⱼ, -θᵢⱼ - γᵢⱼ])\n",
    "\"\"\"\n",
    "logits_exist = sem_module.adjacency_module.adjacency_distribution.logits_exist\n",
    "logits_orient = sem_module.adjacency_module.adjacency_distribution.logits_orient\n",
    "\n",
    "neg_theta = fill_triangular(logits_orient, upper=True) - fill_triangular(logits_orient, upper=False)\n",
    "logits_matrix =  -torch.logsumexp(\n",
    "                    torch.stack([-logits_exist, neg_theta, neg_theta - logits_exist], dim=-1), dim=-1\n",
    "                )\n",
    "\n",
    "\n",
    "logits_matrix = logits_matrix.cpu().detach().numpy()\n",
    "# take sigmoid of logits_matrix to get the probability matrix\n",
    "prob_matrix = 1 / (1 + np.exp(-logits_matrix))\n",
    "\n",
    "print(prob_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sem_module.adjacency_module.adjacency_distribution.\n",
    "logits_matrix = sem_module()._adjacency_dist.dist._get_independent_bernoulli_logits().cpu().detach().numpy()\n",
    "prob_matrix = 1 / (1 + np.exp(-logits_matrix))\n",
    "prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round the probability matrix to 2 decimal places to get the adjacency matrix\n",
    "adj_matrix = np.round(prob_matrix, 2)\n",
    "\n",
    "adj_matrix_avg[adj_matrix_avg < 0.5] = 0 # Only keep edges with weight >= 0.5\n",
    "\n",
    "graph = nx.from_numpy_array(adj_matrix_avg, create_using=nx.DiGraph)\n",
    "graph = nx.relabel_nodes(graph, dict(enumerate(data_module.dataset_train.keys())))\n",
    "\n",
    "labels = {node: i for i, node in enumerate(graph.nodes)}\n",
    "\n",
    "layout = nx.circular_layout(graph)\n",
    "# layout = nx.nx_agraph.graphviz_layout(graph, prog=\"dot\")\n",
    "\n",
    "fig, axis = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "for node, i in labels.items():\n",
    "    axis.scatter(layout[node][0], layout[node][1], label=f\"{i}: {node}\")\n",
    "\n",
    "# add legend to right side of the plot outside the plot area\n",
    "axis.legend(loc=\"center left\", bbox_to_anchor=(1.05, 0.5))\n",
    "\n",
    "nx.draw_networkx(graph, pos=layout, with_labels=True, arrows=True, labels=labels, ax=axis)  \n",
    "\n",
    "# Get weights of each edge and assign to labels\n",
    "labels = nx.get_edge_attributes(graph, \"weight\")\n",
    "\n",
    "# Draw edge labels using layout and list of labels\n",
    "nx.draw_networkx_edge_labels(graph, pos=layout, edge_labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate Prediction Scores from the learned SEM\n",
    "Use appropriate evaluation metrics based on whether the variable is binary or continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions of each variable from the model\n",
    "test_data=data_module._dataset_train\n",
    "prediction = sem.func(test_data, sem.graph)\n",
    "prediction = sem.noise_dist(prediction).mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming y_true and y_pred are the ground truth and predicted labels respectively\n",
    "y_true = test_data[\"Discount\"].detach().numpy()\n",
    "y_pred = prediction[\"Discount\"].detach().numpy()\n",
    "\n",
    "# Compute classification report\n",
    "report = classification_report(y_true, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"\\nClassification report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Assuming y_true and y_pred are the ground truth and predicted labels respectively\n",
    "y_true = test_data[\"Revenue\"].detach().numpy()\n",
    "y_pred = prediction[\"Revenue\"].detach().numpy()\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse) # or use mean_squared_error(y_true, y_pred, squared=False)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print('Mean Absolute Error (MAE): {}'.format(mae))\n",
    "print('Mean Squared Error (MSE): {}'.format(mse))\n",
    "print('Root Mean Squared Error (RMSE): {}'.format(rmse))\n",
    "print('R² Score: {}'.format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted vs ground truth values for the continuous variable \n",
    "# Create a new figure\n",
    "plt.figure()\n",
    "# Scatter plot of y_true vs y_pred\n",
    "plt.scatter(y_true, y_pred, color='blue', alpha=0.25, label='y_true vs y_pred')\n",
    "# Line plot for y=x line\n",
    "x = np.linspace(min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max()), 100)\n",
    "plt.plot(x, x, '--', color='red', label='y=x line')\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel('Ground Truth')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title(\"Revenue\")\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "f3b890945f0a2a7d66e5073e7e9bcbc2851f8572ae8f0cdfdffaf04595a394fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
